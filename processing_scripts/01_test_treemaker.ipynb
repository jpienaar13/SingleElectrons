{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lax Version : 1.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/lgrandi/anaconda3/envs/pax_head/lib/python3.4/site-packages/seaborn-0.8.1-py3.4.egg/seaborn/apionly.py:6: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "run ../../initialize.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Pre_trigger_peaks import Pre_Trigger\n",
    "from Pre_trigger_peaks import Primary_Times\n",
    "#from PI_after_s2 import S2_ionization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = hax.runs.datasets \n",
    "datasets = hax.runs.tags_selection(include=['sciencerun1*'],\n",
    "                                  exclude=['bad','messy', 'test',\n",
    "                                           'nofield','lowfield',\n",
    "                                           'commissioning',\n",
    "                                           'pmttrip','trip','_pmttrip',\n",
    "                                           'source_opening',\n",
    "                                           ],\n",
    "                                  )\n",
    "datasets= hax.cuts.selection(datasets, datasets['location'] != '', 'Processed data available')\n",
    "\n",
    "#Radon\n",
    "datasets_rn = hax.cuts.selection(datasets, datasets['source__type']=='Rn220', 'Source in place')\n",
    "dataset_names_rn = datasets_rn['name']\n",
    "\n",
    "#Bkg\n",
    "datasets_bkg = hax.cuts.selection(datasets, datasets['source__type']=='none', 'Source in place')\n",
    "dataset_names_bkg = datasets_bkg['name']\n",
    "\n",
    "#Krypton\n",
    "datasets_kr = hax.cuts.selection(datasets, datasets['source__type']=='Kr83m', 'Source in place')\n",
    "dataset_names_kr = datasets_kr['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_names_bkg.iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 170415_1228: Making Pre_Trigger minitree: 100%|██████████| 19642/19642 [01:50<00:00, 177.14it/s]\n",
      "Run 170415_1228: Making Pre_Trigger minitree: 100%|██████████| 19642/19642 [01:56<00:00, 168.19it/s]\n"
     ]
    }
   ],
   "source": [
    "df = hax.minitrees.load(\"170415_1228\",\n",
    "                         treemakers=[Pre_Trigger],\n",
    "                         preselection=None,\n",
    "                         force_reload=True,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = hax.minitrees.load(\"170415_1228\",\n",
    "                         treemakers=[Pre_Trigger, Primary_Times],\n",
    "                         preselection=None,\n",
    "                         force_reload=True,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Standard Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uncomment aft cut\n",
    "\n",
    "#Run Number to post-process\n",
    "run_number=\"170415_1228\"\n",
    "\n",
    "#Main\n",
    "pax_config = configuration.load_configuration('XENON1T')\n",
    "R_tpc = pax_config['DEFAULT']['tpc_radius']\n",
    "\n",
    "#Load File Location\n",
    "cache_file_name = '/scratch/midway2/jpienaar/cache_files/'+run_number+ '_Pre_trigger.hdf5'\n",
    "print (run_number, cache_file_name)\n",
    "#df = hax.minitrees.load(cache_file = cache_file_name)\n",
    "\n",
    "#Load AFT Cut Values\n",
    "with open('/home/jpienaar/SingleElectrons/aft_fit_values.pkl', 'rb') as handle:\n",
    "    dict_aft_fits = pickle.load(handle)\n",
    "\n",
    "#Define AFT Cut    \n",
    "def aft_peak_cut(df):\n",
    "    aft_means=np.array(dict_aft_fits['Background']['means'])\n",
    "    aft_sigmas=np.array(dict_aft_fits['Background']['sigmas'])\n",
    "    bins=dict_aft_fits['Background']['bins']\n",
    "    df['CutPeakAFT']= True ^ (df.area_fraction_top>np.interp(np.log10(df.area), bins, aft_means-3*aft_sigmas)) ^ (df.area_fraction_top<np.interp(np.log10(df.area), bins, aft_means+3*aft_sigmas)) \n",
    "    return df\n",
    "\n",
    "#Determine Cut Pass/Fail for whole DF\n",
    "df = aft_peak_cut(df)\n",
    "\n",
    "#Binning\n",
    "window_length=5*10**8\n",
    "t_bins     = np.linspace(0, window_length, 501)\n",
    "t_bin_width= t_bins[1:]-t_bins[:-1]\n",
    "r_bins     = np.linspace(0, (R_tpc)**2, 101)\n",
    "dist_bins  = np.linspace(-R_tpc, R_tpc, 101)\n",
    "s2_bins    = np.linspace(2, 6, 51)\n",
    "s2_p_bins  = np.linspace(0, 4, 51)\n",
    "\n",
    "#Define Blank Hists\n",
    "livet_histogram=Histdd(bins=[t_bins,\n",
    "                             s2_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    's2_area',\n",
    "                                    ]) \n",
    "\n",
    "livet_weights_histogram=Histdd(bins=[t_bins,\n",
    "                             s2_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    's2_area',\n",
    "                                    ]) \n",
    "\n",
    "events_histogram=Histdd(bins=[t_bins,\n",
    "                              s2_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    's2_area'\n",
    "                                    ]) \n",
    "\n",
    "weights_histogram=Histdd(bins=[t_bins,\n",
    "                              s2_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    's2_area'\n",
    "                                    ]) \n",
    "\n",
    "\n",
    "peaks_histogram=Histdd(bins=[t_bins,\n",
    "                             s2_p_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    'peak_area'\n",
    "                                    ]) \n",
    "\n",
    "\n",
    "dt_r2_histogram=Histdd(bins=[t_bins,\n",
    "                             r_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    'r_dist',\n",
    "                                    ]) \n",
    "\n",
    "weights_dt_r2_histogram=Histdd(bins=[t_bins,\n",
    "                             r_bins,\n",
    "                             ], \n",
    "                        axis_names=['delta_T',\n",
    "                                    'r_dist',\n",
    "                                    ]) \n",
    "\n",
    "\n",
    "dt_xy_histogram=Histdd(bins=[t_bins,\n",
    "                           dist_bins,\n",
    "                           dist_bins,\n",
    "                           ],\n",
    "                    axis_names=['delta_T',\n",
    "                                 'x_dist',\n",
    "                                 'y_dist',\n",
    "                                 ])\n",
    "\n",
    "#Find all unique primary S2 events\n",
    "#all_events=pd.unique(df['event_number'].values)\n",
    "#unique_s2s=[]\n",
    "#num_s2s=np.min([500000, len(all_events)])\n",
    "#num_s2s=len(all_events)\n",
    "#for key, event in basic_minitree.iterrows():\n",
    "#    unique_s2s.append([event.s2_time, event.x_s2_tpf, event.y_s2_tpf, event.s2])\n",
    "\n",
    "#Load Minitree with Primary S2 information\n",
    "primary_minitree = hax.minitrees.load(run_number,\n",
    "                         treemakers=[Primary_Times, 'Basics'],\n",
    "                         preselection=None,\n",
    "                         force_reload=True,\n",
    "                                   )\n",
    "\n",
    "#For each unique s2 investigate subsquent S2s\n",
    "num_s2s=len(primary_minitree)\n",
    "loop_count=0\n",
    "for key, event in tqdm(primary_minitree.iterrows()):\n",
    "    #window_length=window_length\n",
    "    temp_df = df.loc[(df.global_time>(event.s2_time))&(df.global_time<(event.s2_time+window_length))]\n",
    "    \n",
    "    #Find unique events within 1ms\n",
    "    subsequent_events=pd.unique(temp_df['event_number'].values)\n",
    "    event_info=[]\n",
    "    for event_id in subsequent_events:\n",
    "        event_df=temp_df[temp_df.event_number==event_id].iloc[0]\n",
    "        event_info.append([np.nanmin([event_df.s1_time, event_df.s2_time]), event_df.event_start])\n",
    "    \n",
    "    #Binning Info of events\n",
    "    bin_allocation_start = np.digitize([x[1]-event.s2_time for x in event_info], bins=t_bins)\n",
    "    bin_allocation_end = np.digitize([x[0]-event.s2_time for x in event_info], bins=t_bins)\n",
    "    bin_difference=bin_allocation_end-bin_allocation_start\n",
    "    \n",
    "    #Determine Live_time conisdered in each time bin\n",
    "    start_time=[x[1]-event.s2_time for x in event_info]\n",
    "    end_time=[np.min([x[0]-event.s2_time, window_length]) for x in event_info]\n",
    "    \n",
    "    live_time_array=[]\n",
    "    #Bin number in digitize of by 1\n",
    "    for idx, bin_allocation in enumerate(bin_difference):\n",
    "        if bin_allocation == 0:\n",
    "            live_time_array.append([t_bins[bin_allocation_start[idx]-1], \n",
    "                                    (end_time[idx]-start_time[idx])/t_bin_width[bin_allocation_start[idx]-1]])\n",
    "        elif bin_allocation == 1:\n",
    "            live_time_array.append([t_bins[bin_allocation_start[idx]-1], \n",
    "                                   (t_bins[bin_allocation_start[idx]]-start_time[idx])/t_bin_width[bin_allocation_start[idx]-1]])\n",
    "            if bin_allocation_start[idx]<len(t_bins)-1:\n",
    "                live_time_array.append([t_bins[bin_allocation_start[idx]], \n",
    "                                    (end_time[idx]-t_bins[bin_allocation_start[idx]])/t_bin_width[bin_allocation_start[idx]]])\n",
    "        elif bin_allocation >1:\n",
    "            live_time_array.append([t_bins[bin_allocation_start[idx]-1], \n",
    "                                   (t_bins[bin_allocation_start[idx]]-start_time[idx])/t_bin_width[bin_allocation_start[idx]-1]])\n",
    "            for index in range(bin_allocation-1):\n",
    "                if bin_allocation_start[idx]+index<len(t_bins)-1:\n",
    "                    live_time_array.append([t_bins[bin_allocation_start[idx]+index],1])\n",
    "            if bin_allocation_start[idx]+(bin_allocation-1)<len(t_bins)-1:\n",
    "                live_time_array.append([t_bins[bin_allocation_start[idx]+(bin_allocation-1)], \n",
    "                                        (end_time[idx]-t_bins[bin_allocation_start[idx]+(bin_allocation-1)])/t_bin_width[bin_allocation_start[idx]+(bin_allocation-1)]])\n",
    "    \n",
    "    #Apply AFT Cut\n",
    "    #temp_df=hax.cuts.selection(temp_df, temp_df['CutPeakAFT'], \"CutPeakAFT\")\n",
    "    \n",
    "    #Some Extra Branches\n",
    "    r_s2=event.x_s2_tpf**2+event.y_s2_tpf**2\n",
    "    \n",
    "    temp_df['x_dist'] = temp_df['x_p_tpf']-event.x_s2_tpf\n",
    "    temp_df['y_dist'] = temp_df['y_p_tpf']-event.y_s2_tpf\n",
    "    temp_df['r_dist'] = np.sqrt(temp_df['x_dist']**2+temp_df['y_dist']**2)\n",
    "    \n",
    "    temp_df['alpha'] = np.arccos((temp_df['r_dist']**2+r_s2**2-R_tpc**2)/(2*r_s2*temp_df['r_dist']))\n",
    "    temp_df['area_norm'] = temp_df['alpha'].fillna(np.pi) \n",
    "    temp_df['norm'] = temp_df['area_norm']/np.pi\n",
    "    \n",
    "    #Binning for bin width rate correction\n",
    "    peak_bins=np.digitize(temp_df['global_time'].values-event.s2_time, t_bins)\n",
    "  \n",
    "    \n",
    "    #Fill Live_T Histogram\n",
    "    histogram=Histdd([x[0]+1 for x in live_time_array],\n",
    "                     [np.log10(event.s2)]*len(live_time_array),\n",
    "                     weights=[x[1] for x in live_time_array],\n",
    "                     axis_names=['delta_T',\n",
    "                                 's2_area',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           s2_bins,\n",
    "                           ])\n",
    "    livet_histogram+=histogram\n",
    "\n",
    "    #Fill Live_T Histogram\n",
    "    histogram=Histdd([x[0]+1 for x in live_time_array],\n",
    "                     [np.log10(event.s2)]*len(live_time_array),\n",
    "                     weights=[x[1]**2 for x in live_time_array],\n",
    "                     axis_names=['delta_T',\n",
    "                                 's2_area',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           s2_bins,\n",
    "                           ])\n",
    "    livet_weights_histogram+=histogram\n",
    "\n",
    "    \n",
    "    #Fill Events Histogram according to Peak Size\n",
    "    histogram=Histdd(temp_df['global_time'].values-event.s2_time,\n",
    "                     np.log10(temp_df['area'].values),\n",
    "                     weights=1/t_bin_width[peak_bins-1],\n",
    "                     axis_names=['delta_T',\n",
    "                                 'peak_area',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           s2_p_bins,\n",
    "                           ])\n",
    "    peaks_histogram+=histogram\n",
    "    \n",
    "    #Fill Events Histogram according to S2 Size\n",
    "    histogram=Histdd(temp_df['global_time'].values-event.s2_time,\n",
    "                     [np.log10(event.s2)]*len(temp_df),\n",
    "                     weights=1/t_bin_width[peak_bins-1],\n",
    "                     axis_names=['delta_T',\n",
    "                                 's2_area',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           s2_bins,\n",
    "                           ])\n",
    "    events_histogram+=histogram\n",
    "    \n",
    "    #Fill weights histogram for Events Histogram according to S2 Size\n",
    "    histogram=Histdd(temp_df['global_time'].values-event.s2_time,\n",
    "                     [np.log10(event.s2)]*len(temp_df),\n",
    "                     weights=(1/t_bin_width[peak_bins-1])**2,\n",
    "                     axis_names=['delta_T',\n",
    "                                 's2_area',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           s2_bins,\n",
    "                           ])\n",
    "    weights_histogram+=histogram\n",
    "            \n",
    "    ##Fill TR_Dist Histogram\n",
    "    histogram=Histdd(temp_df['global_time'].values-event.s2_time,\n",
    "                     (temp_df['r_dist'].values)**2,\n",
    "                     weights=1/temp_df['norm'].values,\n",
    "                     axis_names=['delta_T',\n",
    "                                 'r_dist',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           r_bins,\n",
    "                           ])\n",
    "    dt_r2_histogram+=histogram\n",
    "    \n",
    "    ##Fill Weight TR_Dist Histogram\n",
    "    histogram=Histdd(temp_df['global_time'].values-event.s2_time,\n",
    "                     (temp_df['r_dist'].values)**2,\n",
    "                     weights=(1/temp_df['norm'].values)**2,\n",
    "                     axis_names=['delta_T',\n",
    "                                 'r_dist',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           r_bins,\n",
    "                           ])\n",
    "    weights_dt_r2_histogram+=histogram\n",
    "    \n",
    "    ##Fill TXY_Dist Histogram\n",
    "    histogram=Histdd(temp_df['global_time'].values-event.s2_time,\n",
    "                     temp_df['x_dist'].values,\n",
    "                     temp_df['y_dist'].values,\n",
    "                     axis_names=['delta_T',\n",
    "                                 'x_dist',\n",
    "                                 'y_dist',\n",
    "                                 ], \n",
    "                     bins=[t_bins,\n",
    "                           dist_bins,\n",
    "                           dist_bins,\n",
    "                           ])\n",
    "    dt_xy_histogram+=histogram\n",
    "    \n",
    "    loop_count+=1\n",
    "    if loop_count>5000:\n",
    "        break\n",
    "        \n",
    "#Store in Dict for Later\n",
    "dict_hist={'version' : 1.0,\n",
    "           'deltat':events_histogram, 'deltat_weights':weights_histogram,\n",
    "           'peaks': peaks_histogram, \n",
    "           'livet_hist': livet_histogram, 'livet_weights': livet_weights_histogram, \n",
    "           'dt_r2':dt_r2_histogram, 'dt_r2_weights':weights_dt_r2_histogram,\n",
    "           'dt_xy':dt_xy_histogram, 'events': num_s2s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "new_hist=dict_hist['deltat'].sum('s2_area')\n",
    "new_hist.plot()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "new_hist=dict_hist['livet_hist'].sum('s2_area')\n",
    "new_hist.plot()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "new_hist=dict_hist['deltat'].sum('s2_area')/dict_hist['livet_hist'].sum('s2_area')*1000000\n",
    "new_hist.plot()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Treemaker Code Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import hax\n",
    "import numpy as np\n",
    "from pax import units\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def yield_peak_info(event, event_info):  \n",
    "    #Get all non-lone-hit peaks in the TPC\n",
    "    peaks_tmp = []\n",
    "    for i, peak in enumerate(event.peaks):\n",
    "        if i == event_info.i_s2 or i == event_info.i_s1  or (peak.n_hits<10 and peak.type!='s1'):\n",
    "            continue\n",
    "        else:\n",
    "            peaks_tmp.append(peak) \n",
    "    \n",
    "    peaks = [p for p in peaks_tmp if p.detector == 'tpc' and p.type!='lone_hit' and p.type!='unknown']\n",
    "    \n",
    "    #Ordering by left bound time [10ns]\n",
    "    peaks = sorted(peaks, key=lambda p: p.left) \n",
    "    \n",
    "    #If peaks list is empty, nothing to do\n",
    "    if not len(peaks):\n",
    "        return []\n",
    "    \n",
    "    for i, peak in enumerate(peaks):      \n",
    "        #Make sure event.peak is to the right of S1/S2 by looking at time (ns)\n",
    "        time_peak = peak.area_midpoint\n",
    "        if not np.isnan(event_info.time_s1):\n",
    "            if not time_peak < event_info.time_s1: \n",
    "                continue\n",
    "            else:\n",
    "                yield peak\n",
    "        if np.isnan(event_info.time_s1):\n",
    "            if not time_peak < event_info.time_s2: \n",
    "                continue\n",
    "            else:\n",
    "                yield peak\n",
    "\n",
    "class Event_Info:\n",
    "    def __init__ (self):\n",
    "        self.x_s2_tpf = np.nan\n",
    "        self.y_s2_tpf = np.nan\n",
    "        self.x_s2_nn = np.nan\n",
    "        self.y_s2_nn = np.nan\n",
    "        self.time_s2 = np.nan\n",
    "        self.time_s1 = np.nan\n",
    "        self.i_s2 = np.nan\n",
    "        self.i_s1 = np.nan\n",
    "        self.first_trigger = np.nan\n",
    "\n",
    "                \n",
    "class Pre_Trigger(hax.minitrees.MultipleRowExtractor):\n",
    "    __version__ = '6.0.0'\n",
    "    uses_arrays=True\n",
    "    #extra_branches = ['peaks.left', 'peaks.n_hits', 'peaks.area', 'peaks.type',\n",
    "    #                  'peaks.n_contributing_channels', 'peaks.n_contributing_channels_top',\n",
    "    #                  'peaks.reconstructed_positions*', 'peaks.area_midpoint']\n",
    "    extra_branches = ['*']\n",
    " \n",
    "    def extract_data(self, event):       \n",
    "        results = []\n",
    "        \n",
    "        #Initialize Data Structure for storing event level information\n",
    "        event_info = Event_Info()\n",
    "        \n",
    "        #If we have an interaction get both S1 and S2 information\n",
    "        if len(event.interactions):\n",
    "            #Get indices of S1/S2 in primary interaction\n",
    "            event_info.i_s2=event.interactions[0].s2\n",
    "            event_info.i_s1=event.interactions[0].s1\n",
    "\n",
    "            #Get time of S1/S2 from start of event\n",
    "            event_info.time_s1=event.peaks[event_info.i_s1].area_midpoint\n",
    "            event_info.time_s2=event.peaks[event_info.i_s2].area_midpoint\n",
    "        #If no interaction use largest S2 greater than 150 PE as \"primary S2\"\n",
    "        elif len(event.s2s):\n",
    "            if event.s2s[0]>150:\n",
    "                event_info.i_s2 = event.s2s[0]\n",
    "                \n",
    "                event_info.time_s2 = event.peaks[event_info.i_s2].area_midpoint\n",
    "\n",
    "        #Get S2 positions (TPF/NN) of primary S2\n",
    "        if np.isnan(event_info.i_s2) == False:\n",
    "            rp_s2 = event.peaks[event_info.i_s2].reconstructed_positions\n",
    "            for rp in rp_s2:\n",
    "                if rp.algorithm == 'PosRecTopPatternFit':\n",
    "                    event_info.x_s2_tpf = rp.x\n",
    "                    event_info.y_s2_tpf = rp.y\n",
    "                elif rp.algorithm == 'PosRecNeuralNet':\n",
    "                    event_info.x_s2_nn = rp.x\n",
    "                    event_info.y_s2_nn = rp.y\n",
    "        \n",
    "        #Identify first peak large enough to be considered a trigger.\n",
    "        peaks_trigger = [p.area_midpoint for p in event.peaks if p.type=='s1' or p.area>150]\n",
    "        event_info.first_trigger = min(peaks_trigger, default=1.0e6)   \n",
    "        \n",
    "        \n",
    "        peak_branches = ['area', 'area_fraction_top', 'n_hits', \n",
    "                         'n_contributing_channels', 'n_contributing_channels_top']\n",
    "        for peak in yield_peak_info(event, event_info):\n",
    "            result = dict({x: getattr(peak, x) for x in peak_branches})\n",
    "            result['x_s2_tpf'] = event_info.x_s2_tpf\n",
    "            result['y_s2_tpf'] = event_info.y_s2_tpf\n",
    "            result['x_s2_nn'] = event_info.x_s2_nn\n",
    "            result['y_s2_nn'] = event_info.y_s2_nn\n",
    "            result['global_time'] = peak.area_midpoint + event.start_time\n",
    "            result['event_stop'] = event.stop_time\n",
    "            result['event_start'] = event.start_time\n",
    "            result['s1_time'] = event_info.time_s1+event.start_time\n",
    "            result['s2_time'] = event_info.time_s2+event.start_time  \n",
    "            result['p_range_50p_area'] = peak.range_area_decile[5]\n",
    "            result['p_range_90p_area'] = peak.range_area_decile[9]\n",
    "            result['rise_time'] = peak.area_decile_from_midpoint[1]\n",
    "            result['time_before_trigger'] = event_info.first_trigger - peak.area_midpoint\n",
    "            result['window_length'] = event_info.first_trigger\n",
    "            for rp in peak.reconstructed_positions:\n",
    "                if rp.algorithm == 'PosRecTopPatternFit':\n",
    "                    result['x_p_tpf'] = rp.x\n",
    "                    result['y_p_tpf'] = rp.y\n",
    "                    result['xy_gof_tpf'] = rp.goodness_of_fit\n",
    "                elif rp.algorithm == 'PosRecNeuralNet':\n",
    "                    result['x_p_nn'] = rp.x\n",
    "                    result['y_p_nn'] = rp.y\n",
    "                    result['xy_gof_nn'] = rp.goodness_of_fit        \n",
    "            if peak.type == 's1': \n",
    "                result['type'] = 1\n",
    "            if peak.type == 's2': \n",
    "                result['type'] = 2\n",
    "            if peak.type == 'lone_hit': \n",
    "                result['type'] = 3\n",
    "            if peak.type == 'unknown': \n",
    "                result['type'] = 4   \n",
    "            results.append(result)\n",
    "        \n",
    "        #If no peak objects, still need to store S1/S2 info:\n",
    "        if not results:\n",
    "            result = dict({x: np.nan for x in peak_branches})\n",
    "            result['x_s2_tpf'] = event_info.x_s2_tpf\n",
    "            result['y_s2_tpf'] = event_info.y_s2_tpf\n",
    "            result['x_s2_nn'] = event_info.x_s2_nn\n",
    "            result['y_s2_nn'] = event_info.y_s2_nn\n",
    "            result['global_time'] = np.nan\n",
    "            result['event_stop'] = event.stop_time\n",
    "            result['event_start'] = event.start_time\n",
    "            result['s1_time'] = event_info.time_s1+event.start_time\n",
    "            result['s2_time'] = event_info.time_s2+event.start_time  \n",
    "            result['p_range_50p_area'] = np.nan\n",
    "            result['p_range_90p_area'] = np.nan\n",
    "            result['rise_time'] = np.nan\n",
    "            result['time_before_trigger'] = np.nan\n",
    "            result['window_length'] = event_info.first_trigger\n",
    "            result['x_p_tpf'] = np.nan\n",
    "            result['y_p_tpf'] = np.nan\n",
    "            result['xy_gof_tpf'] = np.nan\n",
    "            result['x_p_nn'] = np.nan\n",
    "            result['y_p_nn'] = np.nan\n",
    "            result['xy_gof_nn'] = np.nan        \n",
    "            result['type'] = np.nan   \n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = hax.minitrees.load(\"170415_1228\",\n",
    "                         treemakers=[Pre_Trigger],\n",
    "                         preselection=None,\n",
    "                         force_reload=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.hist('rise_time', bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pax import units, configuration, datastructure\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from multihist import Histdd, Hist1d\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from Pre_trigger_peaks import Primary_Times\n",
    "\n",
    "#Run Number to post-process\n",
    "run_number=dataset=sys.argv[1]\n",
    "\n",
    "#Init Hax and Studd\n",
    "hax.init(experiment='XENON1T', \n",
    "        pax_version_policy = '6.8.0',\n",
    "        main_data_paths = ['/project2/lgrandi/xenon1t/processed', '/project/lgrandi/xenon1t/processed'],\n",
    "        minitree_paths = ['/scratch/midway2/jpienaar/minitrees/',\n",
    "                         '/project2/lgrandi/xenon1t/minitrees/pax_v6.8.0',\n",
    "                         '/project/lgrandi/xenon1t/minitrees/pax_v6.8.0',\n",
    "                         ],\n",
    "        ) \n",
    "    \n",
    "#Set window to look for previous S2s\n",
    "window_length=10**9\n",
    "\n",
    "df=df\n",
    "\n",
    "#Load Minitree with Primary S2 information\n",
    "primaries = hax.minitrees.load(\"170415_1228\",\n",
    "                         treemakers=[Primary_Times, 'Basics'],\n",
    "                         preselection=None,\n",
    "                         force_reload=True,\n",
    "                                   )\n",
    "\n",
    "\n",
    "pre_existing_fields = []\n",
    "for field in df.columns:\n",
    "    pre_existing_fields.append(field)\n",
    "primary_fields=['x_dist', 'y_dist', 'delay', 's2']\n",
    "\n",
    "\n",
    "#For each unique s2 investigate subsquent S2s\n",
    "num_s2s=len(primaries)\n",
    "loop_count=0\n",
    "new_df=[]\n",
    "for key, event in tqdm(df.iterrows()):    \n",
    "    #window_length=window_length\n",
    "    #temp_primaries = primaries.loc[(primaries.s2_time<(event.global_time))&(primaries.s2_time>(event.global_time-window_length))]\n",
    "    primaries['delay'] = event.global_time - primaries.s2_time\n",
    "    temp_primaries = primaries.loc[(primaries.delay<window_length)&(primaries.delay>0)]\n",
    "    #print(len(temp_primaries))\n",
    "    temp_primaries = temp_primaries.sort_values(by=['delay'])\n",
    "    temp_primaries['x_dist'] = temp_primaries['x_s2_tpf'] - event.x_p_tpf\n",
    "    temp_primaries['y_dist'] = temp_primaries['y_s2_tpf'] - event.y_p_tpf\n",
    "\n",
    "    row_entry={}\n",
    "    \n",
    "    for field in pre_existing_fields:\n",
    "        row_entry[field]=event[field]\n",
    "        \n",
    "    primary_index=1\n",
    "    for key, primary in temp_primaries.iterrows():\n",
    "        for field in primary_fields:\n",
    "            row_entry['%i_%s' %(primary_index, field)]=primary[field]\n",
    "        primary_index+=1\n",
    "    \n",
    "    new_df.append(row_entry)\n",
    "    loop_count+=1\n",
    "    if loop_count>1000:\n",
    "        break\n",
    "\n",
    "new_df = pd.DataFrame(new_df)\n",
    "\n",
    "#new_df.to_hdf('/scratch/midway2/jpienaar/cache_files/%s_Delay.hdf5' %(run_number), key='df') \n",
    "\n",
    "#with open('/scratch/midway2/jpienaar/cache_files/%s_dt.pkl' %run_number, 'wb') as handle:\n",
    "#with open('/scratch/midway2/jpienaar/cache_files/%s.pkl' %run_number, 'wb') as handle:\n",
    "#    pickle.dump(dict_hist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df.hist('2_delay', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
